{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GTX 980M (CNMeM is disabled, cuDNN not available)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from parser import *\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout ,LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "import numpy as np\n",
    "import random \n",
    "text = parse_folder('TheVGLC-master/Super Mario Bros/Processed/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 175440\n",
      "total chars: 15\n"
     ]
    }
   ],
   "source": [
    "print('corpus length:', len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "\n",
    "#Make vocabularies\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 58467\n"
     ]
    }
   ],
   "source": [
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we've read in the text, found out the size of our vocabulary, and split it into semi-redundant sequences.  No we encode it as a 1-hot encoding.\n",
    "\n",
    "This means that if before it looked like:\n",
    "\n",
    "    \n",
    "    -X-X\n",
    "    \n",
    "    \n",
    "and we have the vocab `{'-':0, 'X':1, 'S':2}`\n",
    "\n",
    "it will now look like:\n",
    "\n",
    "    [[1,0,0],[0,1,0],[1,0,0],[0,1,0]]\n",
    "    \n",
    "i.e.  the index of the character in the vocab is set to 1 and everything else is set to 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create two matrices, one of size:\n",
    "    # of sequences X index in sequence X size of vocab\n",
    "    \n",
    "and one of size:\n",
    "    # of sequences X size of vocab\n",
    "    \n",
    "The first is the sequence data in one-hot encoding, and the second is what we are predicting, i.e. the next character in the sequence after the preceding sequence.\n",
    "\n",
    "\n",
    "At this point, we're going to create our Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "size = 128\n",
    "layers = 2\n",
    "dropout = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    size = # of LSTM cells per layer in the neural network\n",
    "    layers = # of layers of LSTM cells\n",
    "    dropout = % of cells to dropout at each training instance \n",
    "    \n",
    "It tends to be a bit of blackart in determining what the proper tuning for the parameters are. Generally, you can assume that bigger is better, and deeper is better, but the balance between the two is up in the air.  It's easier to go deeper than wider, since while a 256 x 2 network has the same number of cells as a 128 x 4 network, it has ~4/3 the number of parameters (~256^2 vs ~128^2 * 3), but there are diminishing returns in both.  \n",
    "\n",
    "Dropout randomly turns off a % of cells for each training instance, which acts as a form of regularization that prevents the network from overfitting.  The reason for this is that instead of specific cells becoming overly attuned, it creates exponentially many sub-networks that must all try to learn the same things in different ways.  Increasing dropout increases training time so it's best to start small, if a divergence between training and validation error appears, increase the dropout and start again.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "#INPUT\n",
    "model.add(LSTM(size, input_shape=(maxlen, len(chars)),return_sequences=True))\n",
    "model.add(Dropout(dropout))\n",
    "#MIDDLE LAYERS\n",
    "for ii in range(layers-2):\n",
    "    model.add(LSTM(size, input_shape=(maxlen,size),return_sequences=True))\n",
    "    model.add(Dropout(dropout))\n",
    "#OUTPUT\n",
    "model.add(LSTM(size, input_shape=(maxlen, len(chars)),return_sequences=False))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model construction is broken into 3 sections:\n",
    "#### INPUT\n",
    "Since this is the first layer in our network, we have to specify the input dimensions (coming from the dimensions of our X data above). \n",
    "#### MIDDLE\n",
    "Here is where we construct an arbitrary number of LSTM layers.  Each of these returns a sequence of vectors (as the layers increase we can think of them learning a hierarchy of sequences)\n",
    "#### OUTPUT\n",
    "Our final LSTM layer doesn't output a sequence and instead outputs a single vector.  This can be thought of as a distillation of the previous sequence into one piece of information.  This is then fed into a Densely connected layer the size of our vocabulary.  The output of this Dense layer has a softmax activation which is defined as:\n",
    "\n",
    "$\\sigma (\\mathbf {z} )_{j}={\\frac {e^{z_{j}}}{\\sum _{k=1}^{K}e^{z_{k}}}}$    for j = 1, â€¦, K.\n",
    "\n",
    "i.e. we exponentiate each output of the Dense layer and then divide by the sum of those exponentiations.\n",
    "\n",
    "Why?  \n",
    "\n",
    "By exponentiating we guarantee that each value > 0.  By dividing by the sum, we guarantee that everything sums to 1.  These are the things we need for discrete probability distribution.  In essence we've now distilled our sequence into a probability distribution over the next character in the sequence given the preceding sequence i.e.\n",
    "\n",
    "$Pr(c_i | c_{i-1},c_{i-2}, ..., c_{i-N})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we need to compile our model.\n",
    "\n",
    "To do so we need 3 things:\n",
    "\n",
    "##### learning rate\n",
    "The rate at which the backpropagation of the error gradient occurs\n",
    "##### optimization technique \n",
    "Stochastic Gradient Descent (SGD) is the core of all the techniques, but there are number of improved techniques.  RMSprop is the de facto choice for Recurrent networks.\n",
    "##### loss criterion\n",
    "Neural networks are just big functions.  SGD + Back Propagation is how we update the parameters of the function, but to do so we need to know how to update our function.  To know how to update we need to know how wrong we are (or how right we are), for categorical distributions this works out to the categorical cross entropy which is defined as:\n",
    "\n",
    "$H(p,q)=-\\sum _{x}p(x)\\,\\log q(x)$\n",
    "\n",
    "Where $p(x)$ is the probability associated with the truth (e.g. 1 for the character, 0 for everything else) and $q(x)$ is the predicted probability.  We could instead just say that our loss is 1 if we get it wrong and 0 if we get it right, but this doesn't reward how confident we are in our predictions, hence why we use $q(x)$ instead of just the predicted character.  \n",
    "\n",
    "NOTE: This is sometimes incorrectly labeled as Softmax loss (it is always coupled with a Softmax activation, but Softmax is the activation and cross entropy is the loss). \n",
    "\n",
    "NOTE: I use the phrase loss criterion, but you will see it called just loss, just criterion, or objective in different places.  These all mean the same thing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "learning_rate = 0.005\n",
    "\n",
    "optimizer = RMSprop(lr=learning_rate)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer,metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------\n",
      "Iteration 1\n",
      "Train on 52620 samples, validate on 5847 samples\n",
      "Epoch 1/1\n",
      "52620/52620 [==============================] - 22s - loss: 0.1596 - acc: 0.9589 - val_loss: 0.1529 - val_acc: 0.9533\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 2\n",
      "Train on 52620 samples, validate on 5847 samples\n",
      "Epoch 1/1\n",
      "52620/52620 [==============================] - 22s - loss: 0.1557 - acc: 0.9602 - val_loss: 0.1560 - val_acc: 0.9581\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 3\n",
      "Train on 52620 samples, validate on 5847 samples\n",
      "Epoch 1/1\n",
      "52620/52620 [==============================] - 23s - loss: 0.1543 - acc: 0.9608 - val_loss: 0.1529 - val_acc: 0.9545\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 4\n",
      "Train on 52620 samples, validate on 5847 samples\n",
      "Epoch 1/1\n",
      "52620/52620 [==============================] - 22s - loss: 0.1521 - acc: 0.9612 - val_loss: 0.1648 - val_acc: 0.9483\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 5\n",
      "Train on 52620 samples, validate on 5847 samples\n",
      "Epoch 1/1\n",
      "52620/52620 [==============================] - 23s - loss: 0.1479 - acc: 0.9618 - val_loss: 0.1433 - val_acc: 0.9583\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 6\n",
      "Train on 52620 samples, validate on 5847 samples\n",
      "Epoch 1/1\n",
      "52620/52620 [==============================] - 23s - loss: 0.1468 - acc: 0.9626 - val_loss: 0.1425 - val_acc: 0.9572\n",
      "\n",
      "----- diversity: 0\n",
      "----- Generating with seed: \"\n",
      "(-------------X(-------------X(--------\"\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "XXXXXXXXXXXXXXXXXXXX\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"\n",
      "(-------------X(-------------X(--------\"\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "-----------------SS-\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "------------S-------\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "XXXXXXXXXXXXXXXXXXXX\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 7\n",
      "Train on 52620 samples, validate on 5847 samples\n",
      "Epoch 1/1\n",
      "52620/52620 [==============================] - 23s - loss: 0.1467 - acc: 0.9625 - val_loss: 0.1373 - val_acc: 0.9613\n",
      "\n",
      "----- diversity: 0\n",
      "----- Generating with seed: \"\n",
      "(-------------X(-------------X(--------\"\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "XXXXXXXXXXXXXXXXXXXX\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"\n",
      "(-------------X(-------------X(--------\"\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "--------------E-----\n",
      "----------SSS-SSS---\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "----------SSSSSSSSSS\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "XXXXXXXXXXXXXXXXXXXX\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 8\n",
      "Train on 52620 samples, validate on 5847 samples\n",
      "Epoch 1/1\n",
      "52620/52620 [==============================] - 23s - loss: 0.1439 - acc: 0.9635 - val_loss: 0.1609 - val_acc: 0.9495\n",
      "\n",
      "----- diversity: 0\n",
      "----- Generating with seed: \"\n",
      "(-------------X(-------------X(--------\"\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "XXXXXXXXXXXXXXXXXXXX\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"\n",
      "(-------------X(-------------X(--------\"\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "---X-------------o--\n",
      "------------------o-\n",
      "--------------------\n",
      "--o-----------E-----\n",
      "----XXX------<>-----\n",
      "-------oE----[]-----\n",
      "-------->----[]---XX\n",
      "XXX----XXXXXXXX-----\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 9\n",
      "Train on 52620 samples, validate on 5847 samples\n",
      "Epoch 1/1\n",
      "52620/52620 [==============================] - 23s - loss: 0.1435 - acc: 0.9632 - val_loss: 0.1389 - val_acc: 0.9569\n",
      "\n",
      "----- diversity: 0\n",
      "----- Generating with seed: \"\n",
      "(-------------X(-------------X(--------\"\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "XXXXXXXXXXXXXXXXXXXX\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"\n",
      "(-------------X(-------------X(--------\"\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "------------X-------\n",
      "------E---X-X-------\n",
      "-----<>---XXX-------\n",
      "-----[]---XXX-------\n",
      "-----[]---XXX-------\n",
      "XXXXXXXXXXXXXXXXXXXX\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# helper function to sample an index from a probability array\n",
    "def sample(preds, temperature=1.0):\n",
    "    if temperature == 0.0:\n",
    "        return np.argmax(preds)\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "# train the model, output generated text after each iteration\n",
    "for iteration in range(1, 10):\n",
    "    print()\n",
    "    print('-' * 50)\n",
    "    print('Iteration', iteration)\n",
    "    model.fit(X, y, batch_size=256, nb_epoch=1, validation_split=0.1)\n",
    "    start_index = 0 \n",
    "    if iteration > 5:\n",
    "        for diversity in [0, 1.0]:\n",
    "            print()\n",
    "            print('----- diversity:', diversity)\n",
    "\n",
    "            generated = ''\n",
    "            sentence = text[start_index: start_index + maxlen]\n",
    "            generated += sentence\n",
    "            print('----- Generating with seed: \"' + sentence + '\"')\n",
    "            \n",
    "\n",
    "            for i in range(300-maxlen+1):\n",
    "                x = np.zeros((1, maxlen, len(chars)))\n",
    "                for t, char in enumerate(sentence):\n",
    "                    x[0, t, char_indices[char]] = 1.\n",
    "\n",
    "                preds = model.predict(x, verbose=0)[0]\n",
    "                next_index = sample(preds, diversity)\n",
    "                next_char = indices_char[next_index]\n",
    "\n",
    "                generated += next_char\n",
    "                sentence = sentence[1:] + next_char\n",
    "\n",
    "            columns = generated.split('(')[1:]\n",
    "\n",
    "            level = [['' for c in columns] for r in columns[0]]\n",
    "\n",
    "            for col_index,column in enumerate(columns):\n",
    "\n",
    "                for row_index, tile in enumerate(column):\n",
    "                    if row_index < len(level) and col_index < len(level[0]):\n",
    "                        level[row_index][col_index] = tile\n",
    "            print('\\n'.join([''.join([tile for tile in row]) for row in level]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
